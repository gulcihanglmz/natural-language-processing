# -*- coding: utf-8 -*-
"""transformers_based_text _representation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L8zzMNdDTc2xnbFNmcV9cJr8oI58bkhk
"""

'''
1-Metnini alıyorsun.Tokenizer ile sayılara çeviriyorsun.
2-Bu sayısal veriyi BERT’e veriyorsun.
3-BERT sana anlamsal vektörler döndürüyor. (her token için 768 boyutlu sayı dizileri — yani embedding'ler)

BERT çıktısı nedir?
BERT tahmin yapmıyor.
"Bu kelimenin anlamını şöyle hesapladım" diyor.
Her token için 768 boyutlu bir vektör dönüyor.
Sen diyorsun ki:
"Ben bu vektörlerle ne yapacağımı biliyorum."
Yani senin eline ham anlam bilgisi geçiyor.

 Özet:
✔️ Sen metni sayıya çeviriyorsun
✔️ BERT sana token embedding'leri veriyor
✔️ Bu embedding'lerle ne yapmak istiyorsan sen karar veriyorsun

👉 BERT = "Ben anlam bilgilerini işledim, işte vektörler kardeşim."
👉 Sen = "Bu vektörlerden duygu mu bakayım, benzerlik mi, kategori mi? Ben karar vereceğim."
'''

from transformers import AutoTokenizer, AutoModel
import torch

from transformers import AutoTokenizer, AutoModel

# Load tokenizer and model
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

text = "Transformers can be used for natural processing"

inputs = tokenizer(text, return_tensors="pt")

'''
Text değişkeninde tuttuğun ham metni (string) alıp, modelin anlayabileceği sayısal token'lara çeviriyor.
* Kelimeleri, alt kelimeleri ya da karakter parçalarını ID'lere dönüştürüyor.
* Aynı zamanda attention mask gibi gerekli şeyleri de hazırlıyor.
* return_tensors="pt" diyerek, sonucu PyTorch tensörü olarak döndürüyor

Alt kelime(subword): "merhaba" → ["mer", "##haba"]

"Merhaba" sözlükte varsa tek parça alıyor.
"Merhabacım" sözlükte yoksa → ["Merhaba", "##cım"] gibi bölüyor.
'''

# BERT Modelinde sadece tahmin almak için
with torch.no_grad():
  outputs = model(**inputs)

# grad:gradyanların hesaplanması durdurulur, bellek verimliliği
#(Bunun için gradyan hesaplamaya gerek yok diyoruz, sadece sonuç gelsin diyoruz.)

last_hidden_state = outputs.last_hidden_state # tüm token çıktılarını almak için

'''
Modele verdiğim her token için son katmandaki çıktı vektörlerini bana ver.

Yani:
Diyelim ki metninde 5 token var.

BERT her token için mesela 768 boyutunda bir vektör üretir.

Bu last_hidden_state tensorü şu şekildedir:
[batch_size, token_sayısı, vektör_boyutu]
[1, 5, 768]

'''

first_token_embedding = last_hidden_state[:, 0, :] # ilk token çıktısını almak için

print(first_token_embedding)

