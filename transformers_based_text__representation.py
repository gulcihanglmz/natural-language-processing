# -*- coding: utf-8 -*-
"""transformers_based_text _representation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L8zzMNdDTc2xnbFNmcV9cJr8oI58bkhk
"""

'''
1-Metnini alÄ±yorsun.Tokenizer ile sayÄ±lara Ã§eviriyorsun.
2-Bu sayÄ±sal veriyi BERTâ€™e veriyorsun.
3-BERT sana anlamsal vektÃ¶rler dÃ¶ndÃ¼rÃ¼yor. (her token iÃ§in 768 boyutlu sayÄ± dizileri â€” yani embedding'ler)

BERT Ã§Ä±ktÄ±sÄ± nedir?
BERT tahmin yapmÄ±yor.
"Bu kelimenin anlamÄ±nÄ± ÅŸÃ¶yle hesapladÄ±m" diyor.
Her token iÃ§in 768 boyutlu bir vektÃ¶r dÃ¶nÃ¼yor.
Sen diyorsun ki:
"Ben bu vektÃ¶rlerle ne yapacaÄŸÄ±mÄ± biliyorum."
Yani senin eline ham anlam bilgisi geÃ§iyor.

 Ã–zet:
âœ”ï¸ Sen metni sayÄ±ya Ã§eviriyorsun
âœ”ï¸ BERT sana token embedding'leri veriyor
âœ”ï¸ Bu embedding'lerle ne yapmak istiyorsan sen karar veriyorsun

ğŸ‘‰ BERT = "Ben anlam bilgilerini iÅŸledim, iÅŸte vektÃ¶rler kardeÅŸim."
ğŸ‘‰ Sen = "Bu vektÃ¶rlerden duygu mu bakayÄ±m, benzerlik mi, kategori mi? Ben karar vereceÄŸim."
'''

from transformers import AutoTokenizer, AutoModel
import torch

from transformers import AutoTokenizer, AutoModel

# Load tokenizer and model
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

text = "Transformers can be used for natural processing"

inputs = tokenizer(text, return_tensors="pt")

'''
Text deÄŸiÅŸkeninde tuttuÄŸun ham metni (string) alÄ±p, modelin anlayabileceÄŸi sayÄ±sal token'lara Ã§eviriyor.
* Kelimeleri, alt kelimeleri ya da karakter parÃ§alarÄ±nÄ± ID'lere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yor.
* AynÄ± zamanda attention mask gibi gerekli ÅŸeyleri de hazÄ±rlÄ±yor.
* return_tensors="pt" diyerek, sonucu PyTorch tensÃ¶rÃ¼ olarak dÃ¶ndÃ¼rÃ¼yor

Alt kelime(subword): "merhaba" â†’ ["mer", "##haba"]

"Merhaba" sÃ¶zlÃ¼kte varsa tek parÃ§a alÄ±yor.
"MerhabacÄ±m" sÃ¶zlÃ¼kte yoksa â†’ ["Merhaba", "##cÄ±m"] gibi bÃ¶lÃ¼yor.
'''

# BERT Modelinde sadece tahmin almak iÃ§in
with torch.no_grad():
  outputs = model(**inputs)

# grad:gradyanlarÄ±n hesaplanmasÄ± durdurulur, bellek verimliliÄŸi
#(Bunun iÃ§in gradyan hesaplamaya gerek yok diyoruz, sadece sonuÃ§ gelsin diyoruz.)

last_hidden_state = outputs.last_hidden_state # tÃ¼m token Ã§Ä±ktÄ±larÄ±nÄ± almak iÃ§in

'''
Modele verdiÄŸim her token iÃ§in son katmandaki Ã§Ä±ktÄ± vektÃ¶rlerini bana ver.

Yani:
Diyelim ki metninde 5 token var.

BERT her token iÃ§in mesela 768 boyutunda bir vektÃ¶r Ã¼retir.

Bu last_hidden_state tensorÃ¼ ÅŸu ÅŸekildedir:
[batch_size, token_sayÄ±sÄ±, vektÃ¶r_boyutu]
[1, 5, 768]

'''

first_token_embedding = last_hidden_state[:, 0, :] # ilk token Ã§Ä±ktÄ±sÄ±nÄ± almak iÃ§in

print(first_token_embedding)

